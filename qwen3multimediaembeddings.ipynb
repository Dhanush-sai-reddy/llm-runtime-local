{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhOG6o+kNOlJSl/myonGLB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanush-sai-reddy/llm-runtime-local/blob/main/qwen3multimediaembeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch pillow opencv-python-headless numpy langgraph langchain-core transformers qwen-vl-utils\n",
        "\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!git clone https://github.com/QwenLM/Qwen3-VL-Embedding.git\n",
        "os.chdir('/content/Qwen3-VL-Embedding')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/Qwen3-VL-Embedding')\n",
        "\n",
        "import torch\n",
        "from src.models.qwen3_vl_embedder import Qwen3VLEmbedder\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "embed_model = Qwen3VLEmbedder(model_name_or_path=\"Qwen/Qwen3-VL-Embedding-2B\")\n",
        "\n",
        "vl_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "vl_proc = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "class State(TypedDict):\n",
        "    video_path: str\n",
        "    query: str\n",
        "    frames: list\n",
        "    descriptions: list\n",
        "    embeddings: list\n",
        "    results: str\n",
        "\n",
        "def extract_frames(state: State):\n",
        "    cap = cv2.VideoCapture(state[\"video_path\"])\n",
        "    frames = []\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(video_fps)\n",
        "    count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % interval == 0:\n",
        "            frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return {\"frames\": frames}\n",
        "\n",
        "def describe_frames(state: State):\n",
        "    descriptions = []\n",
        "    for frame in state[\"frames\"]:\n",
        "        msgs = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": frame}, {\"type\": \"text\", \"text\": \"Describe this.\"}]}]\n",
        "        txt = vl_proc.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "        img_in, vid_in = process_vision_info(msgs)\n",
        "        inputs = vl_proc(text=[txt], images=img_in, videos=vid_in, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = vl_model.generate(**inputs, max_new_tokens=100)\n",
        "        descriptions.append(vl_proc.batch_decode(out, skip_special_tokens=True)[0].split(\"assistant\")[-1].strip())\n",
        "\n",
        "    return {\"descriptions\": descriptions}\n",
        "\n",
        "def create_embeddings(state: State):\n",
        "    embeddings = []\n",
        "    for frame, desc in zip(state[\"frames\"], state[\"descriptions\"]):\n",
        "        inputs = {\"documents\": [{\"image\": frame, \"text\": desc}]}\n",
        "        emb = embed_model.encode(inputs)\n",
        "        embeddings.append(emb[0])\n",
        "\n",
        "    return {\"embeddings\": embeddings}\n",
        "\n",
        "def search_and_answer(state: State):\n",
        "    query_inputs = {\"documents\": [{\"text\": state[\"query\"]}]}\n",
        "    query_emb = embed_model.encode(query_inputs)[0]\n",
        "\n",
        "    sims = [np.dot(query_emb, e) / (np.linalg.norm(query_emb) * np.linalg.norm(e)) for e in state[\"embeddings\"]]\n",
        "    top3 = np.argsort(sims)[-3:][::-1]\n",
        "\n",
        "    results = f\"Query: {state['query']}\\n\\n\"\n",
        "    for idx in top3:\n",
        "        results += f\"Frame {idx}: {state['descriptions'][idx][:100]}\\nSimilarity: {sims[idx]:.3f}\\n\\n\"\n",
        "\n",
        "    return {\"results\": results}\n",
        "\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"extract\", extract_frames)\n",
        "workflow.add_node(\"describe\", describe_frames)\n",
        "workflow.add_node(\"embed\", create_embeddings)\n",
        "workflow.add_node(\"search\", search_and_answer)\n",
        "\n",
        "workflow.set_entry_point(\"extract\")\n",
        "workflow.add_edge(\"extract\", \"describe\")\n",
        "workflow.add_edge(\"describe\", \"embed\")\n",
        "workflow.add_edge(\"embed\", \"search\")\n",
        "workflow.add_edge(\"search\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "result = app.invoke({\n",
        "    \"video_path\": \"/content/video.mp4\",\n",
        "    \"query\": \"What is happening?\"\n",
        "})\n",
        "\n",
        "print(result[\"results\"])"
      ],
      "metadata": {
        "id": "tczI9u1_zvgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}